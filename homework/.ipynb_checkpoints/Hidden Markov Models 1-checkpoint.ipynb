{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  A Hidden Markov Model (HMM) consists of a secret markov process that transitions from state to state. In each state, it outputs an observation, which is all we can see. The classical example is the weather. Suppose the weather is a Markov process with states Rainy and Sunny. However, instead of directly observing the weather, we observe the type of exercize someone does (say either Running, Jogging, or Swimming).<br/>\n",
    " Formally, a HMM consists of $\\pi$, the initial state probabilities, A, the transition matrix for the hidden Markov process, and B, a matrix which describes the probability of observing outputs given the hidden state.<br/>\n",
    " In particular:<br/>\n",
    " $A_{ij}$ is the probability of transitioning from state j to i.<br/>\n",
    " $B_{ki}$ is the probability of observing symbol k given we are in state i.<br/>\n",
    " $\\pi_i$ is the probability of starting in state i.<br/>\n",
    " Given an observation sequence, there are three fundamental problems.<br/>\n",
    " (i) Given an HMM, determine the probability of a particular observation sequence.<br/>\n",
    " (ii) Determine the most likely state sequence given a model and observation sequence.<br/>\n",
    " (iii) Given an observation sequence, recover A, B, and $\\pi$.<br/>\n",
    " \n",
    " Below is a Python class designed to solve these three problems. It will provide the basis for the rest of our examples.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "from itertools import product\n",
    "from numpy.random import normal,random\n",
    "import numpy.linalg as la\n",
    "import sklearn.utils.linear_assignment_ as las\n",
    "\n",
    "\n",
    "def perturbed_uniform_init(shape, sum_axis=0, var = 3*1e-1):\n",
    " mat = np.ones(shape) + var * (random(shape) -.5)\n",
    " #force the matrix to be column or row stochastic depending on sum_axis\n",
    " # sum_axis = 0 corresponds to column stochastic matrices, 1 to row\n",
    " mat /= np.sum(mat, axis = sum_axis)\n",
    " return mat \n",
    "\n",
    "class HMM():\n",
    " def __init__(self,hidden_alphabet, observed_alphabet, A=None,B=None, p=None):\n",
    "  self.A = A\n",
    "  self.B = B\n",
    "  self.p = p\n",
    "  self.hidden = hidden_alphabet\n",
    "  self.observed = observed_alphabet\n",
    "  self.nstates = len(hidden_alphabet)\n",
    "  self.noutputs = len(observed_alphabet)\n",
    "  self.state_list = [None] * self.nstates\n",
    "  for state in self.hidden:\n",
    "    self.state_list[self.hidden[state]] = state\n",
    "\n",
    " def is_not_init(self):\n",
    "  return self.A is None or self.B is None or self.p is None\n",
    "\n",
    " def check_init(self):\n",
    "  if self.is_not_init():\n",
    "   raise ValueError(\"Model parameters have not been initialized.\")\n",
    "\n",
    " def naive_prob(self,observed_sequence):\n",
    "  self.check_init()\n",
    "  prob = 0\n",
    "  l = len(self.hidden)\n",
    "  num_observations = len(observed_sequence)\n",
    "  obs_inds = self.observation_inds(observed_sequence)\n",
    "  for state_sequence in product(range(l), repeat = num_observations):\n",
    "    increment = self.p[state_sequence[0]] * self.B[obs_inds[0], state_sequence[0]]\n",
    "    for i in xrange(1,num_observations):\n",
    "     last_state, curr_state = state_sequence[i-1], state_sequence[i]\n",
    "     increment *= self.A[curr_state, last_state]\n",
    "     increment *= self.B[obs_inds[i], curr_state]\n",
    "    prob += increment \n",
    "  return prob\n",
    "\n",
    " def observation_inds(self, observations):\n",
    "  if type(observations[0]) == int or type(observations[0]) == np.int64:\n",
    "   return observations\n",
    "  return [self.observed[o] for o in observations]\n",
    "   \n",
    " def smart_prob(self, observations):\n",
    "   alpha = self.alpha_pass(observations)\n",
    "   return np.sum(alpha[-1,:])\n",
    "\n",
    "\n",
    " def convert_to_states(self, state_inds):\n",
    "   return [self.state_list[i] for i in state_inds]\n",
    "\n",
    " def viturbi_seq(self, observations):\n",
    "   \"\"\"\n",
    "   Returns the best state sequence in the DP sense\n",
    "   \"\"\"\n",
    "   num_observations = len(observations)\n",
    "   obs_inds = self.observation_inds(observations)\n",
    "   d = np.log(self.p * self.B[0,:])\n",
    "   paths = np.zeros((num_observations, self.nstates), dtype=int)\n",
    "   state_seq = np.array(num_observations)\n",
    "   for t in xrange(1, num_observations):\n",
    "     d_new = np.zeros(self.nstates)\n",
    "     for i in xrange(self.nstates):\n",
    "       temp = np.log(self.A[i,:])  +  d + log(self.B[obs_inds[t], i])\n",
    "       best_previous_state = np.argmax(temp)\n",
    "       d_new[i] = temp[best_previous_state]\n",
    "       paths[t-1, i] = best_previous_state\n",
    "     d = d_new\n",
    "   \n",
    "   best_final_state = np.argmax(d)  \n",
    "   res = np.zeros(num_observations, dtype=int)\n",
    "   res[-1] = best_final_state\n",
    "   last_best = best_final_state\n",
    "   for i in xrange(num_observations - 2, -1, -1):\n",
    "     last_best = paths[i, last_best]\n",
    "     res[i] = last_best\n",
    "   return self.convert_to_states(res) \n",
    "\n",
    " def alpha_pass(self, observations, normalize = False):\n",
    "  obs_inds = self.observation_inds(observations)\n",
    "  nobs = len(obs_inds)\n",
    "  alpha = np.zeros((nobs, self.nstates))\n",
    "  alpha[0,:] = self.B[obs_inds[0],:] * self.p\n",
    "  if normalize:\n",
    "    c_arr = np.zeros(nobs)\n",
    "    c_arr[0] = np.sum(alpha[0,:])\n",
    "    self.logprob = log(c_arr[0])\n",
    "    alpha[0,:] /= c_arr[0]\n",
    "  A,B = self.A, self.B\n",
    "  for i in xrange(1, nobs):\n",
    "    alpha[i,:] = B[obs_inds[i],:] * np.dot(A, alpha[i-1,:])\n",
    "    if normalize:\n",
    "      c_arr[i] = np.sum(alpha[i,:])\n",
    "      self.logprob += log(c_arr[i])\n",
    "      alpha[i,:] /= c_arr[i]\n",
    "  if normalize:\n",
    "    return alpha, c_arr\n",
    "  else:\n",
    "    return alpha\n",
    "\n",
    " def beta_pass(self, obs_inds, normalize = False, c_arr = None):\n",
    "  nobs = len(obs_inds)\n",
    "  beta = np.ones((nobs, self.nstates))\n",
    "  if normalize:\n",
    "    beta[-1,:] /= c_arr[-1]\n",
    "#    c = np.sum(beta[-1,:])\n",
    " #   self.logprob += log(c)\n",
    "  #  beta[-1,:] /= c\n",
    "\n",
    "  for i in xrange(1, nobs):\n",
    "    beta[nobs-i-1,:] = self.B[obs_inds[nobs-i],:] * np.dot(self.A.T, beta[nobs-i,:])\n",
    "    if normalize:\n",
    "      beta[nobs-i-1,:] /= c_arr[nobs-i-1]\n",
    "#       c = np.sum(beta[nobs-i-1,:])\n",
    " #      self.logprob += log(c)\n",
    "  #     beta[nobs-i-1,:] /= c\n",
    "\n",
    "  return beta\n",
    "   \n",
    " def best_seq(self, observations, normalize = False):\n",
    "   \"\"\"\n",
    "   Return the state sequence that maximizes the number of correct states.\n",
    "   \"\"\"\n",
    "   #make sure is numeric\n",
    "   obs = self.observation_inds(observations)\n",
    "   if normalize:\n",
    "     alpha, c_arr = self.alpha_pass(obs, normalize=True)\n",
    "     beta = self.beta_pass(obs, c_arr=c_arr, normalize=True)\n",
    "   else:\n",
    "     alpha, beta = self.alpha_pass(obs), self.beta_pass(obs)\n",
    "   return self.convert_to_states(np.argmax(alpha*beta, axis = 1))\n",
    " \n",
    " def init_random(self):\n",
    "   if self.A is None:\n",
    "     self.A = perturbed_uniform_init((self.nstates, self.nstates))\n",
    "   if self.B is None:\n",
    "     self.B = perturbed_uniform_init((self.noutputs, self.nstates))\n",
    "   if self.p is None:\n",
    "     self.p = perturbed_uniform_init(self.nstates, sum_axis = 0)\n",
    "\n",
    "  \n",
    " def train(self, observations, iters = 50, update_A = True, update_B = True, update_p = True):\n",
    "   O = self.observation_inds(observations)\n",
    "   T = len(O)\n",
    "   oldprob = -np.inf\n",
    "   if self.is_not_init():\n",
    "    self.init_random()\n",
    " #  print self.A\n",
    "   for it in xrange(iters):\n",
    "     alpha, c_arr = self.alpha_pass(O,normalize=True)\n",
    "     beta = self.beta_pass(O, normalize = True, c_arr=c_arr)\n",
    "     digamma = np.zeros((self.nstates, self.nstates, T-1))\n",
    "     gamma = np.zeros((T, self.nstates))\n",
    "     BB = np.zeros((T-1, self.nstates))\n",
    "     for i in xrange(0,T-1):\n",
    "       BB[i,:] = self.B[O[i+1], :] * beta[i+1, :]\n",
    "       digamma[:,:,i] = self.A.T * np.outer(alpha[i,:], BB[i,:])\n",
    "       digamma[:,:,i] /= np.sum(digamma[:,:,i])\n",
    "       gamma[i,:] = np.sum(digamma[:,:,i], axis = 1)   \n",
    "     \n",
    "     gamma[-1, :] = alpha[-1, :]\n",
    "     if update_p:  self.p = gamma[0,:]\n",
    "     if update_A:\n",
    "       for i in xrange(self.nstates):\n",
    "         for j in xrange(self.nstates):\n",
    "           numer = np.sum(digamma[j,i,:T-1])\n",
    "           denom = np.sum(gamma[:T-1,j])          \n",
    "           self.A[i,j] = numer / denom\n",
    "\n",
    "     if update_B:\n",
    "       self.B = np.zeros(self.B.shape)\n",
    "       for i in xrange(T):\n",
    "         self.B[O[i], :] += gamma[i,:]\n",
    "     \n",
    "       for j in xrange(self.nstates):\n",
    "         self.B[:,j] = self.B[:,j] / np.sum(gamma[:,j])\n",
    "     print \"Log Probability: \",self.logprob\n",
    "     if self.logprob < oldprob:\n",
    "       break\n",
    "     else:\n",
    "       oldprob = self.logprob\n",
    "     \n",
    "   print \"Final Probability: \", self.logprob  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consider the problem of determining the temperature of a year by observing the diameter of tree trunks. We model the temperatature as a two-state process with states Hot and Cold. Possible observations are Small, Medium, and Large (for the tree trunk size). Let A, B, and $\\pi$ be as in Problem 1. We seek to compute the probability of observing the sequence M, S, L. <br/>\n",
    " One way is to simply sum over all possible state sequences. This naive approach is accomplished by the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02488\n"
     ]
    }
   ],
   "source": [
    "model = HMM({\"H\":0, \"C\":1}, {\"S\":0, \"M\":1, \"L\": 2})\n",
    "model.A = np.array([[.7,.4],[.3,.6]])\n",
    "model.B = np.array([[.1,.7],[.4,.2],[.5, .1]])\n",
    "model.p = np.array([0., 1.])\n",
    "oseq = [\"M\", \"S\", \"L\"]\n",
    "print model.naive_prob(oseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is exponential in the number of observations, unfortunately. A better way is to use an algorithm known as the $\\alpha$-pass, or the Forward algorithm. This algorithm is only linear in the number of observations, a tremendous improvement!<br/>\n",
    "The algorithm works by computing $\\alpha_t(i)$, the probability of being in state i at time t, and of seeing the given observations $O_1, ..., O_t$ up to time t. It turns out the probability of the entire observation sequence is given by $$P(O) = \\sum_{i=1}^{i=N}\\alpha_{T-1}(i)$$<br/> which follows by marginalizing over the state at time T-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02488\n"
     ]
    }
   ],
   "source": [
    "print model.smart_prob(oseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our attention to the next problem: Computing the best state sequence given observations. It turns out that there are two types of 'best' in this case. The first type of sequence is the sequence that is overall most likely, or the hidden state sequence for which P(O) is maximized. <br/> Of course, we could compute this by simply computing P(O) for every possible state sequence. However, there  is a faster dynamic programming algorithm known as Viturbi's algorithm. This algorithm is implemented in our HMM class and is called below.\n",
    " <br/>\n",
    " The second type of 'best' is the sequence that maximizes the expected number of correct states. At each time step, we choose the hidden state that is most likely for that time step. To compute the probability of being in state i at time t, we need another definition.<br/> Let $$\\beta_t(i)$$ denote the probability of being in state i given the observations after time t, $O_{t+1}, O_{t+2}, ..., O_{T-1}$\n",
    " <br/>This can result in a different answer than viturbi, and is typically more useful in applications.\n",
    " Below is code that solves both problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'H']\n",
      "['C', 'C', 'H']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:70: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "print model.viturbi_seq(oseq) #2(a) Extra Credit! I implemented the viturbi algorithm! \n",
    "print model.best_seq(oseq) #2(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, it turns out that these sequences are the same, but in general they are different problems. Also, don't worry about the runtime error in the above code. It couldn't be helped, and Python was smart enough to deal with it. .<br/>\n",
    "We noe verify that the naive and smart probability computations actually are the same in all cases. We do this by summing the probability of observing a given sequence over all possible sequences, and note that both sums (for the naive and smart methods) are indeed 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum using naive method:  1.0\n",
      "Sum using smart method:  1.0\n"
     ]
    }
   ],
   "source": [
    "model.p = np.array([.6,.4])\n",
    "#Problem 3\n",
    "naive_sum = 0\n",
    "smart_sum = 0\n",
    "for o in product(range(3), repeat=4):\n",
    "  naive_sum += model.naive_prob(o)\n",
    "  smart_sum += model.smart_prob(o)\n",
    "print \"Sum using naive method: \",naive_sum\n",
    "print \"Sum using smart method: \",smart_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the difficult part: training a model given observations. This is accomplished via a form of expectation maximization. In this case, the update rules are:\n",
    "$$\\gamma_t(i,j) = \\alpha_t(i)a_{ji}b_j(O_{t+1})\\beta_{t+1}(j)$$\n",
    "$$\\gamma_t(i) = \\sum_{j=0}^{j=N-1}\\gamma_t(i,j)$$\n",
    "$$\\pi_i = \\gamma_0(i)$$\n",
    "$$a_{ji} = \\frac{\\sum_{t=0}^{T-2}\\gamma_t(i,j)}{\\sum_{t=0}^{T-2}\\gamma_t(i)}$$\n",
    "$$b_j(k) = \\frac{\\sum_{t | O_t = k}\\gamma_t(j)}{\\sum_{t=0}^{T-2}\\gamma_t(j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These can be written in terms of $\\alpha$ and $\\beta$ as follows:\n",
    "        $$\\gamma_t(i) = \\sum_{j=0}^{j=N-1}\\gamma_t(i,j)$$\n",
    "        $$\\pi_i = \\sum_{j=0}^{j=N-1}\\alpha_0(i)a_{ji}b_j(O_{1})\\beta_{1}(j)$$\n",
    "        $$a_{ji} = \\frac{\\sum_{t=0}^{T-2}\\alpha_t(i)a_{ji}b_j(O_{t+1})\\beta_{t+1}(j)}{\\sum_{t=0}^{T-2}\\sum_{j=0}^{j=N-1}\\alpha_t(i)a_{ji}b_j(O_{t+1})\\beta_{t+1}(j)}$$\n",
    "        $$b_j(k) = \\frac{\\sum_{t | O_t = k}\\sum_{j=0}^{j=N-1}\\sum_{i=0}^{i=N-1}\\alpha_t(j)a_{ij}b_i(O_{t+1})\\beta_{t+1}(i)}{\\sum_{t=0}^{T-2}\\sum_{i=0}^{i=N-1}\\alpha_t(j)a_{ij}b_i(O_{t+1})\\beta_{t+1}(i)}$$\n",
    "       \n",
    "These update rules are implemented in the train method of my HMM class. Consider the problem of training an HMM on a corpus of English words, where the observations are the letters. What patterns will it find? When running my HMM on the Brown Corpus with 2, 3, and 4 hidden states, I get the following results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each case, the observation letters correspond to one of the hidden states.\n",
    "2 states:<br/>\n",
    "['a', 'e', 'h', 'i', 'o', 'u', 'y', ' ']<br/>\n",
    "['b', 'c', 'd', 'f', 'g', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v',\n",
    " 'w', 'x', 'z']<br/>\n",
    "\n",
    "Note that the vowels correspond to one state, and the consonants to another. The one mistake is that 'h' is classified as a vowel, which isn't that bad.<br/>\n",
    "\n",
    "3 states:<br/>\n",
    "['a', 'i', 'u', 'x', ' ']<br/>\n",
    "['e', 'g', 'h', 'k', 'o', 'q', 'y']<br/>\n",
    "['b', 'c', 'd', 'f', 'j', 'l', 'm', 'n', 'p', 'r', 's', 't', 'v', 'w', 'z']<br/>\n",
    "\n",
    "The reason 'x' snuck in as a vowel is probably because of the very low frequency of the letter. <br/>\n",
    "\n",
    "4 states:<br/>\n",
    "['d', 'e', 'h', 'n', 's', 'y']<br/>\n",
    "['u', 'x', ' ']<br/>\n",
    "['a', 'i', 'k', 'l', 'o', 'r', 'v']<br/>\n",
    "['b', 'c', 'f', 'g', 'j', 'm', 'p', 'q', 't', 'w', 'z']<br/>\n",
    "\n",
    "Note that ' ' is pretty much in its own class, while most consonants are in their own class.\n",
    "\n",
    "When running the HMM on the cipher-text of problem 10, I get:\n",
    "['a', 'b', 'd', 'e', 'f', 'g', 'i', 'j', 'l', 'm', 'n', 'p', 'q', 'r', 't', 'x', 'y', 'z', ' ']<br/>\n",
    "['c', 'h', 'k', 'o', 's', 'u', 'v', 'w']<br/>\n",
    "\n",
    "The second list of characters presumably corresponds to vowels and the ' ' character in the ciphertext. Indeed, there are 8 characters in the second state, and there are a total of 8 vowels, counting the space character.<br/>\n",
    "\n",
    "Below is a list of helper functions used to generate results for problems 9 and 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(alphabet):\n",
    " nsymbols = len(alphabet)\n",
    " alist = [None] * nsymbols\n",
    " for k in alphabet:\n",
    "   alist[alphabet[k]] = k\n",
    " return alist\n",
    "\n",
    "def print_model_states(model, observation_alphabet):\n",
    " states = [[] for i in xrange(model.nstates)]\n",
    " olist = make_list(observation_alphabet)\n",
    " for i in xrange(len(observation_alphabet)):\n",
    "   states[np.argmax(model.B[i,:])].append(olist[i])\n",
    " for state in states:\n",
    "  print state\n",
    "\n",
    "def alphabet_helper(observations, observation_alphabet, nstates = 2, nobs = 50000, iters = 100, A=None, p =None):\n",
    " hidden_alphabet = {i:i for i in xrange(nstates)}\n",
    " model = HMM(hidden_alphabet, observation_alphabet)\n",
    " model.A = A\n",
    " model.p = p\n",
    " model.train(observations[:nobs], iters = iters)\n",
    " print model.p\n",
    " print model.A\n",
    " print model.B\n",
    " print_model_states(model, observation_alphabet)\n",
    "\n",
    "def parse_char_file(filename):\n",
    " f = open(filename, \"r\")\n",
    " l = f.readline().strip()\n",
    " nobs = len(l)\n",
    " observations = np.zeros(nobs, dtype = int)\n",
    " observation_alphabet = {' ' : 26}\n",
    " for off in xrange(26):\n",
    "   observation_alphabet[chr(ord('a')+off)] = off\n",
    " for i in xrange(nobs):\n",
    "   observations[i] = observation_alphabet[l[i]]\n",
    " return observations, observation_alphabet\n",
    "\n",
    "def alphabet_probs():\n",
    " observations, observation_alphabet = parse_char_file(\"brown.txt\")\n",
    " alphabet_helper(observations, observation_alphabet, 2,50000, iters=100,A = np.array([[.47468,.51656], [.52532,.48344]]), p = np.array([.51316,.48684]))\n",
    " alphabet_helper(observations, observation_alphabet, 3,50000, iters=200)\n",
    " alphabet_helper(observations, observation_alphabet, 4,50000, iters=200)\n",
    " observations, observation_alphabet = parse_char_file(\"ciphertext.txt\")\n",
    " alphabet_helper(observations, observation_alphabet, 2,50000, iters=100,A = np.array([[.47468,.51656], [.52532,.48344]]), p = np.array([.51316,.48684]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
